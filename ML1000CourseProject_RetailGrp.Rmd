---
title: "ML1000CourseProjectRtlGrp"
author: "Ignacio Palma, Jairo Melo and Vikram Khade"
date: '2019-03-13'
output:
  word_document: default
  html_document: default
---

```{r libraries, include=FALSE}
#knitr::opts_chunk$set(echo = FALSE)

library(lattice)
library(ggplot2)
library(caret)
library(randomForest)
library(dplyr)
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering visualization
library(dendextend) # for comparing two dendrograms
library(fpc)
library(stats)    #   clustering algorithms
library(gower) # for using Gower to introduce categorical values with Hierarchical CLustering
library(StatMatch) # for using Gower to introduce categorical values with Hierarchical CLustering
library(NbClust)
library(stringi)
library(RColorBrewer)
library(scales)
library(rpart)
library(rpart.plot)
library(corrplot)

```

## Information Technology Service Management Analysis

ITSM is an area of continues improvement and for major organizations every opportunity could represent major cost savings which translate into more affortable products for patiences and parents.

The file extracted from the ITSM system contains 1.2 year worth data for two major product lines.

## Loading data

You can include R code in the document as follows:

```{r loadingData, echo=FALSE}

getwd();
#setwd('/Users/jairomelo/Desktop/ML/YORK/Assigment2/CourseProject')
sdata=read.csv("~/desktop/ML/YORK/CourseProject/Support Tickets Dataset- ML.csv", header = TRUE, dec = ".")

```

## Cleaning the data
We will take care of duplicated, records with NA values, removing tickets that are not Resolved, as well as undertermine records, for example: Tickets with Support Level outside of the standards.
```{r cleaning, echo=FALSE}
nrow(sdata)

sdata <- select(sdata,-incident)
sdata <- select(sdata,-cust_time)
sdata <- select(sdata,-pend_time)
sdata <- select(sdata,-call_log)
sdata <- select(sdata,-chat_log)
sdata <- select(sdata,-Closed)

#Make sure there are no NA entries.
sum(is.na(sdata))  #0

sdata <- distinct(sdata)
#Removing observations where tickets are not closed
sdata <- sdata[which(sdata$resolved != "" ),]
sdata <- sdata[which(sdata$res_category != "" ),]

#Though sdata is complete, We check if some rows containing NULL or weid entries are caught by this command.
sum(!complete.cases(sdata))

#Removing any other groups that re not under ITSM Governance
sdata <- subset(sdata, grp_level=='Level 1' | grp_level=='Level 2' | grp_level=='Level 3')
sum(sdata$opened == "")
sum(sdata$application == "")
sum(sdata$region == "")
sum(sdata$prod_line == "")
sum(sdata$app_category == "")
sum(sdata$priority == "")
sum(sdata$sup_grp == "")

sum(sdata$grp_level == "")  #1
#Remove rows with blanks
sdata <- sdata[which(sdata$grp_level != "" ),]

sum(sdata$res_category == "") #1
#Remove rows with blanks
sdata <- sdata[which(sdata$res_category != "" ),]

sum(sdata$opened == "")
sum(sdata$urgency == "")
sum(sdata$impact == "")
nrow(sdata)
```


## Data Understanding

* incident: Number of the ticket incident.  Not a significant variable as is sequencial counter.
* application: Number of the application of the reported issue.  This is a relevant variable which a certantly number of tickets are assigned to one application.
* region: Region where the user is located.  Significant as a region is associated to a particular population of users reporting issues of an application.
* prod_line: Product Line is a group of related products under the same brand. For example, Web and Ecommerce, and also Internal Business process applications.
* opened: Date when the issues was opened.  The ticket has 5 stages:  Not Assigned, In Progress, Customer Action, Pending, Resolved, Closed.
Not Assigned: The ticket was created/open, but still not been worked by the support team.
In Progress: The ticket is assigned to a support group who is actively working on it.
Customer Action: The ticket goes into a stand-by because additional information is requested from the user before the current support group can continue working.
Pending: The ticket goes into a stand-by because there is an activity to be performed by a third party group before the current support group can continue working.
Resolved: Once the issue is fixed, the user is notified by the Support team.
Closed: Each resolved ticket moves into Closed after the user confirms, or automatically, the ticket is closed after n number of days.  For our analysis, we will using only tickets that are Resolved.  Closed might not be relevant as there is a strong correlation between Closed and Resolved.
* app_category: Category of the Application.  Relevant as this is the classification of the application.
* priority: Priority of the Issue.  This is the result of Urgency and Impact.  
Low Urgency - Limited Impact = Lower Priority. -> 4
High Urgency - Limite Impact = High Priority. -> 1
The "Priority" word can be removed from the field and use the numbers 1,2,3,4.  Priority 4 is low, and 1 is the highest.
* urgency: How soon the issue should be resolved.  There is a strong correlation between Urgency and Priority; which might cause to ommit the field when using Priority.
* impact: What's the extension of the issue in terms of number of users.  eg: Limited means small group usually 1 or 2 users, Spread-out means usually an area, department or even all organization.  There is a strong correlation between Urgency and Priority; which might cause to ommit the field when using Priority.
* Closed: Date when the ticket was finally closed.  Refer to the Opened field for explanation of the stages of the tickets.
* sup_grp: Support Group providing resolution to the issue.  This is relevant as the support group is responsible to effectively close a ticket as soon as it's assigned.
* grp_level: Support Group Level.  There are 3 different groups of support level.

Level 1: Service Desk, primary group who handles all tickets and try to troubleshot the issue.  Most of the tickets should be filtered by this team.  This is less specialized team, and help to keep Level 2 and 3 focus on major activities.
Level 2: This is the specialist team who has greater knowledge on how the application operates.  This team takes care of tickets Level 1 is not able to resolved.
Level 3: This is the Developers of the applications; has complete knowledge of the application and finally able to resolve the issues scalated by L2 team.

For JnJ, the L2 and L3 are more expensive, and the interest of the company is to identify ways to reduce cost translating activities from L3 to L2 and from L2 to L1.

The "Level" word can be removed from the field and use the numbers 1,2,3.  Level 1 is the less specialized, and 3 is the most specialized, usually a lot more expensive than 1.

* resolved: Date when the issue was resolved.  Refer to the Opened field for explanation of the stages of the tickets.
* res_category: Category of the type of resolution support team completed.
* cust_time: Time in seconds the ticket was waiting for Customer response.  Refer to the Opened field for explanation of the stages of the tickets.
* pend_time: Time ticket is on hold.  Refer to the Opened field for explanation of the stages of the tickets.
* call_log: Id of the phone call When a call is involved.  Not a relevant attribute as not all tickets triggers a phone call.
* chat_log: If of the chat session when user uses instance message with the support team.  This new technology is not heavily used, so there are very few observations with this information.

Let's chart the data to understand more about the variables associated to the support activities

## Data Visualization
Let's review what the data can tell us about supporting applications for JJTS:

### Plot by Application Category
This feature contains great information on how a particular ticket was resolved.
```{r app_category, echo=FALSE}
pp <- ggplot(sdata, aes(x=res_category)) + geom_bar(aes(y=..count../sum(..count..)),col='black',fill='deeppink') + 
  coord_flip()
print(pp)
```

##Exploring Applications
There is more than 500 applications.  This feature might not be the best for Supervised Algorithms.
```{r applications, echo=FALSE}
pp <- ggplot(sdata, aes(x=application)) + geom_bar(aes(y=..count../sum(..count..)),col='violet',fill='violet') + 
  coord_flip()
print(pp)
```


## Exploring Support Group Level
Support Group Level indicates the expertise of the support team.  At the same time, the cost of the time goes up while more expert.
```{r grp_level, echo=FALSE}

pp <- ggplot(sdata, aes(x=grp_level)) + geom_bar(aes(y=..count../sum(..count..)),col='black',fill='deepskyblue') + 
      coord_flip() 
print(pp)

```


## Exploring Application Category
```{r app_catefgory, echo=FALSE}

pp <- ggplot(sdata, aes(x=app_category)) + geom_bar(aes(y=..count../sum(..count..)),col='coral4',fill='coral4') + 
     coord_flip() 
print(pp)
```

##Exploring Support Group
```{r sup_grp, echo=FALSE}
pp <- ggplot(sdata, aes(x=sup_grp)) + geom_bar(aes(y=..count../sum(..count..)),col='coral4',fill='coral4') + 
  coord_flip()
print(pp)
```

##Exploring Priority
Priority is one of the most important features, and it comes from the combination or Urgency and Impact.
```{r priority, echo=FALSE}
pp <- ggplot(sdata, aes(x=priority)) + geom_bar(aes(y=..count../sum(..count..)),col='black',fill='chocolate') + 
   coord_flip()
print(pp)

```

##Exploring Urgency
How soon the issue needs to be resolved.  There is a strong correlation with Priority.
```{r urgency, echo=FALSE}
pp <- ggplot(sdata, aes(x=urgency)) + geom_bar(aes(y=..count../sum(..count..)),col='black',fill='deeppink') + 
  coord_flip()
print(pp)
```

## Exploring Product Line
There are only two product lines in this data set.  Its relevance might not be the highest but we'll keep it while our analysis.
```{r prod_line, echo=FALSE}
pp <- ggplot(sdata, aes(x=prod_line)) + geom_bar(aes(y=..count../sum(..count..)),col='black',fill='firebrick1') + 
  coord_flip()
print(pp)
```

#Exploring the Impact
This is associated to the number of users affected by the issue.
```{r impact, echo=FALSE}
pp <- ggplot(sdata, aes(x=impact)) + geom_bar(aes(y=..count../sum(..count..)),col='black',fill='goldenrod') + 
  coord_flip()
print(pp)
```

#Exploring Region of where the Users are located
When a ticket is created, the location of the user is recorded as well.
```{r region, echo=FALSE}
pp <- ggplot(sdata, aes(x=region)) + geom_bar(aes(y=..count../sum(..count..)),col='black',fill='darkorange') + 
  coord_flip() 
print(pp)
```




## Data Preparation
Below attributes will be removed from the Dataset due to the low analytical value:
Incident: This is the ID of the ticket.  We only use it to ensure there are not duplicates.
cust_time: We will focus on the time that the ticket is resolved, customer time with other teams is not relevant for our analysis
Pend_time: We will focus on the time that the ticket is resolved, pending time with other teams is not relevant for our analysis
call_log: This feature is not used mainly; while Support team uses Skype IM
chat_log: Less than 1% of the tickets are manages through chat from ServiceNow; support team usually use Skype IM, which is not recorded in the dataset.
Closed:  We will focus our analysis on Resolved tickets, close is an automatic process happening 12 days after the ticket was resolved.

## New Numeric Variables
We are now creating Numeric representation of 
Impact -> impactN
Urgency -> urgencyN
Priority -> priorityN
Group Level -> LevelN

```{r numeric, echo=FALSE}

sdata["impactN"] <- "NA"
sdata[sdata$impact=="Limited","impactN"] <- as.numeric(1) 
sdata[sdata$impact=="Large","impactN"] <- as.numeric(2)
sdata[sdata$impact=="Widespread","impactN"] <- as.numeric(3)
sdata <- select(sdata,-impact)

sdata["urgencyN"] <- "NA"
sdata[sdata$urgency=="Low","urgencyN"] <- as.numeric(1) 
sdata[sdata$urgency=="Medium","urgencyN"] <- as.numeric(2)
sdata[sdata$urgency=="High","urgencyN"] <- as.numeric(3)
sdata <- select(sdata,-urgency)

sdata["priorityN"] <- "NA"
sdata[sdata$priority=="Priority 4","priorityN"] <- as.numeric(1) 
sdata[sdata$priority=="Priority 3","priorityN"] <- as.numeric(2)
sdata[sdata$priority=="Priority 2","priorityN"] <- as.numeric(3)
sdata <- select(sdata,-priority)

#Convert ordinal level to numeric and save in new feature
sdata["levelN"] <- as.numeric(0)
sdata[sdata$grp_level=="Level 1","levelN"] <- as.numeric(1) 
sdata[sdata$grp_level=="Level 2","levelN"] <- as.numeric(2)
sdata[sdata$grp_level=="Level 3","levelN"] <- as.numeric(3)
sdata <- select(sdata,-grp_level)

str(sdata)
```


Let's see visually the new Numeric Features
```{r newNumeric, echo=FALSE}

sdata$impactN <- as.numeric(sdata$impactN)
pp <- ggplot(sdata, aes(x=impactN)) + geom_bar(aes(y=..count../sum(..count..)),col='black',fill='goldenrod') + 
  coord_flip()
print(pp)

sdata$priorityN <- as.numeric(sdata$priorityN)
pp <- ggplot(sdata, aes(x=priorityN)) + geom_bar(aes(y=..count../sum(..count..)),col='black',fill='chocolate') + 
  coord_flip()
print(pp)

pp <- ggplot(sdata, aes(x=levelN)) + geom_bar(aes(y=..count../sum(..count..)),col='black',fill='chocolate') + 
  coord_flip()
print(pp)

sdata$urgencyN <- as.numeric(sdata$urgencyN)
pp <- ggplot(sdata, aes(x=urgencyN)) + geom_bar(aes(y=..count../sum(..count..)),col='black',fill='deeppink') + 
  coord_flip()
print(pp)

```

##Dates to chart and Duration of a Ticket

Now we will create the numeric representation of the Date variables and calculate the number of days that support team took to resolve an issue.

ndays is the time support team took to resolved the issue, in this case is calculated as Resolved - Opened
```{r datesChar, echo=FALSE}

#convert dates to date objects
sdata["open_date"] <- "NA"
sdata["open_date"] <- as.Date(as.character(sdata[,"opened"]),"%Y-%m-%d")
sdata <- select(sdata,-opened)
sdata["resolve_date"] <- "NA"
sdata["resolve_date"] <- as.Date(as.character(sdata[,"resolved"]),"%Y-%m-%d")
sdata <- select(sdata,-resolved)
#Calculate days required to resolve a ticket
sdata["ndays"] <- "NA"
sdata["ndays"] <- as.numeric(sdata[,"resolve_date"] - sdata[,"open_date"])

str(sdata)

```

## Correlation Matrix
Based on our previous charts, we are now curious to see if there is any feature which its correlation might cause to have it dropped.

```{r plotting, echo=FALSE}
JJdata.cor = cor(sdata[,c("impactN","urgencyN","priorityN","levelN","ndays")])
JJdata.cor
corrplot(JJdata.cor)
```

From the figure we identify that urgency and priority are strongly correlated
Priority and impact are weakly correlated (0.27) this could be because it is defined by the user during the ticket triaging.
Similarly priority and level are weakly correlated.
ndays has a very low correlation.

Since priority and urgency are highly correlated (0.89) urgency is dropped from further analysis.
```{r dropurgency, echo=FALSE}
sdata <- select(sdata,-urgencyN)

str(sdata)

```



## Low Frequency Cleaning

In particular, App Category, Resolution category and Region contains very low frequency levels which could reduce accuracy for our predictions or computing time during our cluster analys.  We choose the Threshold = 2.5% to remove observations.
```{r lowFrequency, echo=FALSE}
perc <- round(nrow(sdata)*2.5/100)

tt <- table(sdata$app_category)
rare_levels <- names(tt)[tt<perc]
sdata <- subset(sdata,!app_category %in% rare_levels)
sdata$app_category <- factor(sdata$app_category)

tt <- table(sdata$res_category)
rare_levels <- names(tt)[tt<perc]
sdata <- subset(sdata,!res_category %in% rare_levels)
sdata$res_category <- factor(sdata$res_category)

tt <- table(sdata$region)
rare_levels <- names(tt)[tt<perc]
sdata <- subset(sdata,!region %in% rare_levels)
sdata$region <- factor(sdata$region)

summary(sdata)
#17410
nrow(sdata)

```


## Calculating Performance
We are rating the servcies provided by the vendor the following way:
Excellent:  All P1, P2 or P3 tickets resolved within 1 day
Good:       For P2 tickets resolved within 5 Days
Average:    For P2 or P1 Tickets resolved within 10 days
Failed:     For any other ticket is considered Bad performance is rated as failed service.
- P3 resolved in more than 1 day
- P2 resolved in more than 5 days
- P1 resolved in more than 10 days

```{r performance, echo=FALSE}

sdata$performance <- ifelse(sdata$ndays < 1, "Excellent", ifelse(sdata$priorityN == 2 & sdata$ndays <= 5, "Good", ifelse(sdata$priorityN <= 2 & sdata$ndays <= 10, "Average", "Failed")))

sdata$performance <- factor(sdata$performance)

summary(sdata$performance)
#need to figure out why the numbers are not matching to the %
pieGRP <- table(sdata$performance)
pct <- round(pieGRP/sum(pieGRP)*100)
lbls <- paste(names(pieGRP), "\n", sep="")
lbls <- paste(lbls,pct) # add percents to labels 
lbls <- paste(lbls,"%",sep="") # ad % to labels 
pie(pieGRP,labels = lbls, col=rainbow(length(lbls)),main="Performance")
#pieGRP

```




# Supervised Learning

Adding the reason of using the below predictors
priority + grp_level + app_category + res_category + region + prod_line

##Feature selection
Let's run a random forest to quantify the relative importance of these features.  We will use features with less than 50 Levels, so Application and Support Group will go away.  Also Dates should not be considered.  Ndays is not a predictable variable because we can't actually trying to Predict the Performance of resolving a ticket when Duration is provided since we won't know how long the ticket will last unresolved, but we will know what Priority the ticket is raised.

Finally, Resolution category is unknown as we don't know what the issue is.  We can predict based on the resolution.

```{r checkingPredictors, echo=FALSE}
set.seed(719)

rfImp <- randomForest(performance ~ priorityN + levelN + app_category + region + impactN + prod_line + impactN , data = sdata, ntree = 100, importance = TRUE)
importance(rfImp,type=2)
```





As shown in the table, Product Line has the lowest predictable power.  Understandable because there is only two Product Lines; and the decision is either one of the other.  Very limited predictibility power.

# Suppervised Modeling

## Classification and Regression Trees

For our Analysis, we will select 5 of the most predictable features:

Priority, Support Level, App Category, Resolution Category, Region

```{r regTree, echo=FALSE}

TSdata <- sdata[,c("priorityN", "levelN" , "app_category" , "region", "impactN", "performance")]

#write.csv(TSdata,'TSdata.csv')

splitIndex <- createDataPartition(TSdata[,'performance'], p = .75, list = FALSE, times = 1)
trainDF <- TSdata[ splitIndex,]
testDF  <- TSdata[-splitIndex,]

set.seed(152)

rparTree <- rpart(performance ~ ., data=trainDF, control = rpart.control(cp = 0.0001))

printcp(rparTree)

```


The tree has a misclassification rate of 0.66669 * 0.52531 * 100% = 35% in cross-validation (65% of prediction accuracy). 

## Pruning and ploting model

We now pick the tree size that minimizes prediction error which is a misclassification rate
Prediction error rate in training data = Root node error * rel error * 100%
Prediction error rate in cross-validation = Root node error * xerror * 100%
We want the cp value that minimizes the xerror
```{r treePruned, echo=FALSE}
bestcp <- rparTree$cptable[which.min(rparTree$cptable[,"xerror"]),"CP"]
#Prune the tree using the best cp.
rparTree.pruned <- prune(rparTree, cp = bestcp)
plot(rparTree.pruned)
text(rparTree.pruned, cex = 0.8, use.n = TRUE, xpd = TRUE)

```
## Confusion Matrix for Training data

```{r confusionTable, echo=FALSE}

#show the confusion matrix we had 

conf.matrix <- table(trainDF$performance, predict(rparTree.pruned,type="class"))
rownames(conf.matrix) <- paste("Actual", rownames(conf.matrix), sep = ":")
colnames(conf.matrix) <- paste("Pred", colnames(conf.matrix), sep = ":")
print(conf.matrix)
```

## Ploting the Tree Pruned

```{r modeDetail, echo=FALSE}
prp(rparTree.pruned, faclen = 0, cex = 0.8, extra = 1)
```

##Let's test the model (accuracy of testing dataset)

#a) Classification Tree / Recursive Partitioning and Confusion Matrix

```{r prediction, echo=FALSE}
summary(testDF$performance)
predictions <- predict(rparTree, testDF)

summary(predictions)
head(predictions)
#confusionMatrix(predictions, testDF$performance)

```

Ploting the Tree in black in White in case of not having a Color printer
```{r plotTree, echo=FALSE}

tot_count <- function(x, labs, digits, varlen)
{
  paste(labs, "\n\nn =", x$frame$n)
}

prp(rparTree.pruned, faclen = 0, cex = 0.8, node.fun=tot_count)

```

Adding color to the Tree for each Performance

```{r plotColors, echo=FALSE}

only_count <- function(x, labs, digits, varlen)
{
  paste(x$frame$n)
}

boxcols <- c("palegreen3", "blue", "white","pink")[rparTree.pruned$frame$yval]

par(xpd=TRUE)
prp(rparTree.pruned, faclen = 0, cex = 0.8, node.fun=only_count, box.col = boxcols)
legend("bottomleft", legend = c("Excellent","Good","Average","Failed"), fill = c("palegreen3", "blue", "white","pink"),
       title = "Performance")

```

## Deployment
Predicting outcome of an unseen data point
```{r deployment, echo=FALSE}
xnew = sdata[21170,c('priorityN','app_category','levelN','region','impactN')]
predict(rparTree,xnew)


```
