---
title: "ML1000CourseProjectRtlGrp"
author: "Ignacio Palma, Jairo Melo and Vikram Khade"
date: '2019-03-13'
output:
  word_document: default
  html_document: default
---

```{r libraries, include=FALSE}
#knitr::opts_chunk$set(echo = FALSE)

library(lattice)
library(ggplot2)
library(caret)
library(randomForest)
library(dplyr)
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering visualization
library(dendextend) # for comparing two dendrograms
library(fpc)
library(stats)    #   clustering algorithms
library(gower) # for using Gower to introduce categorical values with Hierarchical CLustering
library(StatMatch) # for using Gower to introduce categorical values with Hierarchical CLustering
library(NbClust)
library(stringi)
library(RColorBrewer)
library(scales)
library(rpart)
library(rpart.plot)
library(corrplot)

```

## Information Technology Service Management Analysis

ITSM is an area of continues improvement and for major organizations every opportunity could represent major cost savings which translate into more affortable products for patiences and parents.

The file extracted from the ITSM system contains 1.2 year worth data for two major product lines.

## Loading data

You can include R code in the document as follows:

```{r loadingData, echo=FALSE}

getwd();
#setwd('/Users/jairomelo/Desktop/ML/YORK/Assigment2/CourseProject')
sdata=read.csv("~/desktop/ML/YORK/CourseProject/Support Tickets Dataset- ML.csv", header = TRUE, dec = ".")

```

## Identify Anomalies/Cleaning the data
We will take care of duplicated, records with NA values, removing tickets that are not Resolved, as well as undertermine records, for example: Tickets with Support Level outside of the standards.


```{r cleaning, echo=FALSE}
nrow(sdata)

#Make sure there are no NA entries.
sum(is.na(sdata))  #0

sdata <- distinct(sdata)
#Removing observations where tickets are not closed
sdata <- sdata[which(sdata$resolved != "" ),]
sdata <- sdata[which(sdata$res_category != "" ),]

#Though sdata is complete, We check if some rows containing NULL or weid entries are caught by this command.
sum(!complete.cases(sdata))

#Removing any other groups that re not under ITSM Governance
sdata <- subset(sdata, grp_level=='Level 1' | grp_level=='Level 2' | grp_level=='Level 3')
nrow(sdata)
```

Now we will ensure no data issues to reduce the risk of miss interpretation
We need to remove any of observation if any of the below is greater than 0 Zero

```{r sum_zero, echo=FALSE}

#Adding validation to avoid data issues
sum(sdata$opened == "")
sum(sdata$application == "")
sum(sdata$region == "")
sum(sdata$prod_line == "")
sum(sdata$app_category == "")
sum(sdata$priority == "")
sum(sdata$sup_grp == "")
sum(sdata$grp_level == "")  #1
#Remove rows with blanks
sdata <- sdata[which(sdata$grp_level != "" ),]
sum(sdata$res_category == "") #1
#Remove rows with blanks
sdata <- sdata[which(sdata$res_category != "" ),]
sum(sdata$opened == "")
sum(sdata$urgency == "")
sum(sdata$impact == "")


```


## Data Understanding

* incident: Number of the ticket incident.  Not a significant variable as is sequencial counter.
* application: Number of the application of the reported issue.  This is a relevant variable which a certantly number of tickets are assigned to one application.
* region: Region where the user is located.  Significant as a region is associated to a particular population of users reporting issues of an application.
* prod_line: Product Line is a group of related products under the same brand. For example, Web and Ecommerce, and also Internal Business process applications.
* opened: Date when the issues was opened.  The ticket has 5 stages:  Not Assigned, In Progress, Customer Action, Pending, Resolved, Closed.
Not Assigned: The ticket was created/open, but still not been worked by the support team.
In Progress: The ticket is assigned to a support group who is actively working on it.
Customer Action: The ticket goes into a stand-by because additional information is requested from the user before the current support group can continue working.
Pending: The ticket goes into a stand-by because there is an activity to be performed by a third party group before the current support group can continue working.
Resolved: Once the issue is fixed, the user is notified by the Support team.
Closed: Each resolved ticket moves into Closed after the user confirms, or automatically, the ticket is closed after n number of days.  For our analysis, we will using only tickets that are Resolved.  Closed might not be relevant as there is a strong correlation between Closed and Resolved.
* app_category: Category of the Application.  Relevant as this is the classification of the application.
* priority: Priority of the Issue.  This is the result of Urgency and Impact.  
Low Urgency - Limited Impact = Lower Priority. -> 4
High Urgency - Limite Impact = High Priority. -> 1
The "Priority" word can be removed from the field and use the numbers 1,2,3,4.  Priority 4 is low, and 1 is the highest.
* urgency: How soon the issue should be resolved.  There is a strong correlation between Urgency and Priority; which might cause to ommit the field when using Priority.
* impact: What's the extension of the issue in terms of number of users.  eg: Limited means small group usually 1 or 2 users, Spread-out means usually an area, department or even all organization.  There is a strong correlation between Urgency and Priority; which might cause to ommit the field when using Priority.
* Closed: Date when the ticket was finally closed.  Refer to the Opened field for explanation of the stages of the tickets.
* sup_grp: Support Group providing resolution to the issue.  This is relevant as the support group is responsible to effectively close a ticket as soon as it's assigned.
* grp_level: Support Group Level.  There are 3 different groups of support level.

Level 1: Service Desk, primary group who handles all tickets and try to troubleshot the issue.  Most of the tickets should be filtered by this team.  This is less specialized team, and help to keep Level 2 and 3 focus on major activities.
Level 2: This is the specialist team who has greater knowledge on how the application operates.  This team takes care of tickets Level 1 is not able to resolved.
Level 3: This is the Developers of the applications; has complete knowledge of the application and finally able to resolve the issues scalated by L2 team.

For JnJ, the L2 and L3 are more expensive, and the interest of the company is to identify ways to reduce cost translating activities from L3 to L2 and from L2 to L1.

The "Level" word can be removed from the field and use the numbers 1,2,3.  Level 1 is the less specialized, and 3 is the most specialized, usually a lot more expensive than 1.

* resolved: Date when the issue was resolved.  Refer to the Opened field for explanation of the stages of the tickets.
* res_category: Category of the type of resolution support team completed.
* cust_time: Time in seconds the ticket was waiting for Customer response.  Refer to the Opened field for explanation of the stages of the tickets.
* pend_time: Time ticket is on hold.  Refer to the Opened field for explanation of the stages of the tickets.
* call_log: Id of the phone call When a call is involved.  Not a relevant attribute as not all tickets triggers a phone call.
* chat_log: If of the chat session when user uses instance message with the support team.  This new technology is not heavily used, so there are very few observations with this information.

Let's chart the data to understand more about the variables associated to the support activities

## Data Visualization
Let's review what the data can tell us about supporting applications for JJTS:

### Plot by Application Category
This feature contains great information on how a particular ticket was resolved.
```{r app_category, echo=FALSE}
pp <- ggplot(sdata, aes(x=res_category)) + geom_bar(aes(y=..count../sum(..count..)),col='black',fill='deeppink') + 
  coord_flip()
print(pp)
```

##Exploring Applications
There is more than 500 applications.  This feature might not be the best for Supervised Algorithms.
```{r applications, echo=FALSE}
pp <- ggplot(sdata, aes(x=application)) + geom_bar(aes(y=..count../sum(..count..)),col='violet',fill='violet') + 
  coord_flip()
print(pp)
```


## Exploring Support Group Level
Support Group Level indicates the expertise of the support team.  At the same time, the cost of the time goes up while more expert.
```{r grp_level, echo=FALSE}

pp <- ggplot(sdata, aes(x=grp_level)) + geom_bar(aes(y=..count../sum(..count..)),col='black',fill='deepskyblue') + 
      coord_flip() 
print(pp)

```


## Exploring Application Category
```{r app_catefgory, echo=FALSE}

pp <- ggplot(sdata, aes(x=app_category)) + geom_bar(aes(y=..count../sum(..count..)),col='coral4',fill='coral4') + 
     coord_flip() 
print(pp)
```

##Exploring Support Group
```{r sup_grp, echo=FALSE}
pp <- ggplot(sdata, aes(x=sup_grp)) + geom_bar(aes(y=..count../sum(..count..)),col='coral4',fill='coral4') + 
  coord_flip()
print(pp)
```

##Exploring Priority
Priority is one of the most important features, and it comes from the combination or Urgency and Impact.
```{r priority, echo=FALSE}
pp <- ggplot(sdata, aes(x=priority)) + geom_bar(aes(y=..count../sum(..count..)),col='black',fill='chocolate') + 
   coord_flip()
print(pp)

```

##Exploring Urgency
How soon the issue needs to be resolved.  There is a strong correlation with Priority.
```{r urgency, echo=FALSE}
pp <- ggplot(sdata, aes(x=urgency)) + geom_bar(aes(y=..count../sum(..count..)),col='black',fill='deeppink') + 
  coord_flip()
print(pp)
```

## Exploring Product Line
There are only two product lines in this data set.  Its relevance might not be the highest but we'll keep it while our analysis.
```{r prod_line, echo=FALSE}
pp <- ggplot(sdata, aes(x=prod_line)) + geom_bar(aes(y=..count../sum(..count..)),col='black',fill='firebrick1') + 
  coord_flip()
print(pp)
```

#Exploring the Impact
This is associated to the number of users affected by the issue.
```{r impact, echo=FALSE}
pp <- ggplot(sdata, aes(x=impact)) + geom_bar(aes(y=..count../sum(..count..)),col='black',fill='goldenrod') + 
  coord_flip()
print(pp)
```

#Exploring Region of where the Users are located
When a ticket is created, the location of the user is recorded as well.
```{r region, echo=FALSE}
pp <- ggplot(sdata, aes(x=region)) + geom_bar(aes(y=..count../sum(..count..)),col='black',fill='darkorange') + 
  coord_flip() 
print(pp)
```


## Data Preparation


Below attributes will be removed from the Dataset due to the low analytical value:
Incident: This is the ID of the ticket.  We only use it to ensure there are not duplicates.
cust_time: We will focus on the time that the ticket is resolved, customer time with other teams is not relevant for our analysis
Pend_time: We will focus on the time that the ticket is resolved, pending time with other teams is not relevant for our analysis
call_log: This feature is not used mainly; while Support team uses Skype IM
chat_log: Less than 1% of the tickets are manages through chat from ServiceNow; support team usually use Skype IM, which is not recorded in the dataset.
Closed:  We will focus our analysis on Resolved tickets, close is an automatic process happening 12 days after the ticket was resolved.

Here is the final data set
```{r removefield, echo=FALSE}

#Remove irrelevant features
sdata <- select(sdata,-incident)
sdata <- select(sdata,-cust_time)
sdata <- select(sdata,-pend_time)
sdata <- select(sdata,-call_log)
sdata <- select(sdata,-chat_log)
sdata <- select(sdata,-Closed)
#Ensuring all the observations are valid
sdata <- na.omit(sdata)
str(sdata)

```

## New Numeric Variables
We are now creating Numeric representation of 
Impact -> impactN
Urgency -> urgencyN
Priority -> priorityN
Group Level -> LevelN

```{r numeric, echo=FALSE}

sdata["impactN"] <- "NA"
sdata[sdata$impact=="Limited","impactN"] <- as.numeric(1) 
sdata[sdata$impact=="Large","impactN"] <- as.numeric(2)
sdata[sdata$impact=="Widespread","impactN"] <- as.numeric(3)
sdata <- select(sdata,-impact)

sdata["urgencyN"] <- "NA"
sdata[sdata$urgency=="Low","urgencyN"] <- as.numeric(1) 
sdata[sdata$urgency=="Medium","urgencyN"] <- as.numeric(2)
sdata[sdata$urgency=="High","urgencyN"] <- as.numeric(3)
sdata <- select(sdata,-urgency)

sdata["priorityN"] <- "NA"
sdata[sdata$priority=="Priority 4","priorityN"] <- as.numeric(1) 
sdata[sdata$priority=="Priority 3","priorityN"] <- as.numeric(2)
sdata[sdata$priority=="Priority 2","priorityN"] <- as.numeric(3)
sdata <- select(sdata,-priority)

#Convert ordinal level to numeric and save in new feature
sdata["levelN"] <- as.numeric(0)
sdata[sdata$grp_level=="Level 1","levelN"] <- as.numeric(1) 
sdata[sdata$grp_level=="Level 2","levelN"] <- as.numeric(2)
sdata[sdata$grp_level=="Level 3","levelN"] <- as.numeric(3)
#sdata <- select(sdata,-grp_level)

str(sdata)
```


Let's see visually the new Numeric Features
```{r newNumeric, echo=FALSE}

sdata$impactN <- as.numeric(sdata$impactN)
pp <- ggplot(sdata, aes(x=impactN)) + geom_bar(aes(y=..count../sum(..count..)),col='black',fill='goldenrod') + 
  coord_flip()
print(pp)

sdata$priorityN <- as.numeric(sdata$priorityN)
pp <- ggplot(sdata, aes(x=priorityN)) + geom_bar(aes(y=..count../sum(..count..)),col='black',fill='chocolate') + 
  coord_flip()
print(pp)

pp <- ggplot(sdata, aes(x=levelN)) + geom_bar(aes(y=..count../sum(..count..)),col='black',fill='chocolate') + 
  coord_flip()
print(pp)

sdata$urgencyN <- as.numeric(sdata$urgencyN)
pp <- ggplot(sdata, aes(x=urgencyN)) + geom_bar(aes(y=..count../sum(..count..)),col='black',fill='deeppink') + 
  coord_flip()
print(pp)

```

##Dates to chart and Duration of a Ticket

Now we will create the numeric representation of the Date variables and calculate the number of days that support team took to resolve an issue.

ndays is the time support team took to resolved the issue, in this case is calculated as Resolved - Opened
```{r datesChar, echo=FALSE}

#convert dates to date objects
sdata["open_date"] <- "NA"
sdata["open_date"] <- as.Date(as.character(sdata[,"opened"]),"%Y-%m-%d")
sdata <- select(sdata,-opened)
sdata["resolve_date"] <- "NA"
sdata["resolve_date"] <- as.Date(as.character(sdata[,"resolved"]),"%Y-%m-%d")
sdata <- select(sdata,-resolved)
#Calculate days required to resolve a ticket
sdata["ndays"] <- "NA"
sdata["ndays"] <- as.numeric(sdata[,"resolve_date"] - sdata[,"open_date"])

str(sdata)

```

## Correlation Matrix
Based on our previous charts, we are now curious to see if there is any feature which its correlation might cause to have it dropped.

```{r plotting, echo=FALSE}
JJdata.cor = cor(sdata[,c("impactN","urgencyN","priorityN","levelN","ndays")])
JJdata.cor
corrplot(JJdata.cor)
```

From the figure we identify that urgency and priority are strongly correlated
Priority and impact are weakly correlated (0.27) this could be because it is defined by the user during the ticket triaging.
Similarly priority and level are weakly correlated.
ndays has a very low correlation.

Since priority and urgency are highly correlated (0.89) urgency is dropped from further analysis.
```{r dropurgency, echo=FALSE}
sdata <- select(sdata,-urgencyN)

str(sdata)

```

## Low Frequency Cleaning

In particular, App Category, Resolution category and Region contains very low frequency levels which could reduce accuracy for our predictions or computing time during our cluster analys.  We choose the Threshold = 2.5% to remove observations.
```{r lowFrequency, echo=FALSE}
perc <- round(nrow(sdata)*2.5/100)

tt <- table(sdata$app_category)
rare_levels <- names(tt)[tt<perc]
sdata <- subset(sdata,!app_category %in% rare_levels)
sdata$app_category <- factor(sdata$app_category)

tt <- table(sdata$res_category)
rare_levels <- names(tt)[tt<perc]
sdata <- subset(sdata,!res_category %in% rare_levels)
sdata$res_category <- factor(sdata$res_category)

tt <- table(sdata$region)
rare_levels <- names(tt)[tt<perc]
sdata <- subset(sdata,!region %in% rare_levels)
sdata$region <- factor(sdata$region)

summary(sdata)
nrow(sdata)
```


## Calculating Performance
We are rating the servcies provided by the vendor the following table where the numbers are the days expected to have the issue resolved.

Priority  |P1  |P2  | P3
--------------------------
Excellent | 5  | 3  | 1
Good      |10  | 5  | 3
Average   |15  |10  | 5

Failed:     For any other ticket is considered Bad performance is rated as failed service.

```{r performance, echo=FALSE}
sdata$performance <- ifelse(sdata$ndays < 1, "Excellent", ifelse(sdata$ndays < 3 & sdata$priorityN <= 2, "Excellent", ifelse(sdata$ndays < 5 & sdata$priorityN < 2, "Excellent",ifelse(sdata$ndays < 3, "Good", ifelse(sdata$ndays < 5 & sdata$priorityN <= 2, "Good", ifelse(sdata$ndays < 10 & sdata$priorityN < 2, "Good", ifelse(sdata$ndays < 5, "Average", ifelse(sdata$ndays < 10 & sdata$priorityN <= 2, "Average", ifelse(sdata$ndays < 15 & sdata$priorityN < 2, "Average","Failed")))))))))

sdata$performance <- factor(sdata$performance)

summary(sdata$performance)
#need to figure out why the numbers are not matching to the %
pieGRP <- table(sdata$performance)
pct <- round(pieGRP/sum(pieGRP)*100)
lbls <- paste(names(pieGRP), "\n", sep="")
lbls <- paste(lbls,pct) # add percents to labels 
lbls <- paste(lbls,"%",sep="") # ad % to labels 
pie(pieGRP,labels = lbls, col=rainbow(length(lbls)),main="Performance")

```

##Performance by most significant features
Since now we have performance, let's inspect how the current support teams performs.
##Performance by Application Category
Interesting is that across the applications support team is performing quite well.  Howeverm software shows the highest issues.
```{r performanceapp}
ggplot(data = sdata, mapping = aes(x = performance, y = app_category)) + geom_jitter(aes(colour = performance))
```

##Performance by Support Level
This is the Level of the Support Group level, 1 Service Desk, 2 Specialist, 3 Developer/Architect.  From the below chat we can conclude that most of the tickets are resolved by Support Level 2.  It would be interesting to look for opportunities to move L2 to L1.  Perhaps our unsupervise analysis would provide us ideas.

```{r performancesup}
ggplot(data = sdata, mapping = aes(x = performance, y = levelN)) + geom_jitter(aes(colour = performance))
```
##Performance by Resolution Category
Altough, resolution category can't be use as predictor, it's interesting to visualize Data Issue is the most common across all the performance.
```{r performanceres}
ggplot(data = sdata, mapping = aes(x = performance, y = res_category)) + geom_jitter(aes(colour = performance))
```


# Supervised Modeling


# Supervised Learning - Predicting Group Level

```{r}
```

## Objective
For our analysis, we will predict which group level a ticket will be assigned based on the basic information provided by the User.

## What's the Problem
A ticket usually gets scalated depending on the complexity, the time each Group Level takes to analize the ticket and scalate it could be critical.  Reliability team would like to find a way to know which team would be finally involved in a ticket so they can allocated the resources according to the number of tickets.

##Feature selection
Let's run a random forest to quantify the relative importance of these features.  We will use features with less than 50 Levels, so Application and Support Group will go away.  Also Dates should not be considered.  Duration/Ndays and resolution category are is not predictable variable because we don't know what resolutions category would be or how long the ticket would take.

The final Group Level predictors are:
app_category + prod_line + priorityN + region + impactN

```{r checkingPredictorsLevel, echo=FALSE}
set.seed(719)
sdata$grp_level <- factor(sdata$grp_level)

rfImp <- randomForest(grp_level ~ app_category + prod_line + priorityN + region + impactN, data = sdata, ntree = 100, importance = TRUE)
importance(rfImp,type=2)
```

As shown in the table, Product Line has the lowest predictable power.  Understandable because there is only two Product Lines; and the decision is either one of the other.  Very limited predictability power.

```{r regTreeLevel, echo=FALSE}

TSdata <- sdata[,c("grp_level" , "app_category" , "prod_line" , "priorityN", "region", "impactN")]
#write.csv(TSdata,'TSdata.csv')

splitIndex <- createDataPartition(TSdata[,'grp_level'], p = .75, list = FALSE, times = 1)
trainDF <- TSdata[ splitIndex,]
testDF  <- TSdata[-splitIndex,]
```
Let's see the dimmensions for:
```{r dimensionsLevel}
#Training
dim(trainDF); 

#Test
dim(testDF);

```
All Algorithms

```{r NonlinearLevel, echo=FALSE}
set.seed(152)
trctl <- trainControl(method = 'cv', number = 10, savePredictions = TRUE)
metric <- "Accuracy"
fit.lda <- train(grp_level ~ ., data=trainDF, method="lda", metric=metric, trControl=trctl)
fit.rf <- train(grp_level ~ ., data=trainDF, method="rf", metric=metric, trControl=trctl)
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3, savePredictions = TRUE)
fit.cart <- train(grp_level ~ ., data=trainDF, method="rpart",
                  parms = list(split = "information"),
                  metric=metric,
                  trControl=trctl)

```

As per the table; the mean accuracy of random forest is the highest of all the three algorithms.

```{r Level, echo=FALSE}
results <- resamples(list(lda=fit.lda, cart=fit.cart, rf=fit.rf))
summary(results)
```
## Visualize the accuracy of the models of the Training
```{r}
dotplot(results)
```

The most important metric is prediction on the Testing data

##Let's make a prediction (accuracy of testing dataset)

a) Linear Discriminant Analysis and Confusion Matrix
```{r Levellda, echo=FALSE}
predictions <- predict(fit.lda, testDF)
head(predictions)
confusionMatrix(factor(predictions, levels = c("Level 1","Level 2","Level 3")),factor(testDF$grp_level, levels = c("Level 1","Level 2","Level 3")))

```
The Balance accuracy is less than 81.4% for all performance and with the Balanced accuracy:
           Level 1        Level 2        Level 3
           0.70312         0.6707        0.50000

b) Classification Tree / Recursive Partitioning and Confusion Matrix
```{r levelcart, echo=FALSE}
predictions <- predict(fit.cart, testDF)
head(predictions)
confusionMatrix(factor(predictions, levels = c("Level 1","Level 2","Level 3")),factor(testDF$grp_level, levels = c("Level 1","Level 2","Level 3")))
```
The Balance accuracy for Classification Tree is 81.9% for all performance
           Level 1        Level 2      Level 3
           0.70406         0.6687        0.50000

```{r levelrf, echo=FALSE}
predictions <- predict(fit.rf, testDF)
head(predictions)
confusionMatrix(factor(predictions, levels = c("Level 1","Level 2","Level 3")),factor(testDF$grp_level, levels = c("Level 1","Level 2","Level 3")))

```
The Balance accuracy for Random Forest is 83.6% or accuray
            Level 1        Level 2      Level 3
            0.7349         0.6958      0.5029940
Random forest is giving us the best accuracy of all three methods.




## Plotting Random Forest
```{r plottingcart, echo=FALSE}

print(fit.rf) # view results 
plot(fit.rf)


```

## Deployment of Performance Predictor
We will run a prediction using the Random Forest Model, then we will generate the model to be used by our Shiny App.
```{r deployment_level, echo=FALSE}
saveRDS(fit.rf,'jjts_level_model.rds')

```



## Second Part - Predicting Performance

## Objective
From our previous analisys, we determined the Group Level the ticket will be providing support.  Now, for the second part of our analysis, we will predict what performance a ticket will have given an basic data related to the issue.

## What's the Problem
Business users needs to know how long a ticket would take to be resolved so they can focus on additional activities, or look for workarounds that will reduce the impact of the issue.

##Feature selection
Let's run a random forest to quantify the relative importance of these features.  We will use features with less than 50 Levels, so Application and Support Group will go away.  Also Dates should not be considered.  Ndays is not a predictable variable because we can't actually trying to Predict the Performance of resolving a ticket when Duration is provided since we won't know how long the ticket will last unresolved, but we will know what Priority the ticket is raised.

Finally, Resolution category is unknown as we don't know what the issue is.  We cannot predict based on the resolution.

Summary:
priority + grp_level + app_category + region + prod_line + impactN

```{r checkingPredictors, echo=FALSE}
#we take Group Level Model to add the Performance to the Data Set
#predictLevel <- predict(fit.rf, sdata)
#newdata <- cbind(sdata, predictLevel)  


set.seed(719)

rfImp <- randomForest(performance ~ app_category + levelN + region + priorityN + impactN + prod_line, data = sdata, ntree = 100, importance = TRUE)
importance(rfImp,type=2)
```


As shown in the table, Product Line has the lowest predictable power.  Understandable because there is only two Product Lines; and the decision is either one of the other.  Very limited predictability power.

For our Analysis, we will select 5 of the most predictable features:
Priority, Support Level, App Category, Resolution Category, Region

```{r regTree, echo=FALSE}

TSdata <- sdata[,c("priorityN", "levelN" , "app_category" , "region", "impactN","prod_line", "performance")]

#write.csv(TSdata,'TSdata.csv')

splitIndex <- createDataPartition(TSdata[,'performance'], p = .75, list = FALSE, times = 1)
trainDF <- TSdata[ splitIndex,]
testDF  <- TSdata[-splitIndex,]
```
Let's see the dimmensions for:
```{r dimensions}
#Training
dim(trainDF); 

#Test
dim(testDF);

```
All Algorithms

```{r Nonlinear, echo=FALSE}
set.seed(152)
trctl <- trainControl(method = 'cv', number = 10, savePredictions = TRUE)
metric <- "Accuracy"
fit.lda <- train(performance ~ ., data=trainDF, method="lda", metric=metric, trControl=trctl)
fit.rf <- train(performance ~ ., data=trainDF, method="rf", metric=metric, trControl=trctl)

trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3, savePredictions = TRUE)
fit.cart <- train(performance ~ ., data=trainDF, method="rpart",
                  parms = list(split = "information"),
                  metric=metric,
                  trControl=trctl)

```

As per the table; the mean accuracy of random forest is the highest of all the three algorithms.

```{r}
results <- resamples(list(lda=fit.lda, cart=fit.cart, rf=fit.rf))
summary(results)
```
## Visualize the accuracy of the models of the Training
```{r}
dotplot(results)
```

The most important metric is prediction on the Testing data

##Let's make a prediction (accuracy of testing dataset)

a) Linear Discriminant Analysis and Confusion Matrix
```{r}
predictions <- predict(fit.lda, testDF)
head(predictions)
confusionMatrix(factor(predictions, levels = c("Excellent","Good","Average","Failed")),factor(testDF$performance, levels = c("Excellent","Good","Average","Failed")))

```
The Balance accuracy is less than 70.5% for all performance and with the Balanced accuracy:
           Excellent   Class           Average       Failed
             0.50401    0.499341      0.5032167     0.5017638

b) Classification Tree / Recursive Partitioning and Confusion Matrix
```{r}
predictions <- predict(fit.cart, testDF)
head(predictions)
confusionMatrix(factor(predictions, levels = c("Excellent","Good","Average","Failed")),factor(testDF$performance, levels = c("Excellent","Good","Average","Failed")))
```
The Balance accuracy is 71% for all performance
           Excellent   Class           Average       Failed
             0.51989   0.5026489       0.509343      0.533771
is better than LDA; but not by far.

```{r}
predictions <- predict(fit.rf, testDF)
head(predictions)
confusionMatrix(factor(predictions, levels = c("Excellent","Good","Average","Failed")),factor(testDF$performance, levels = c("Excellent","Good","Average","Failed")))

```
The Balance accuracy is better with Random forest with 71.1% or accuray
           Excellent   Class           Average       Failed
Balanced Accuracy             0.52578    0.502751       0.511204      0.541517
Random forest is giving us the best accuracy of all three, altough, they are pretty close.

## Plotting Classification and Regression Trees (CART)
All three methods are giving us a close prediction accuracy of 71%.  To better understand the process, we will inspect Classification Trees deeper.
```{r plottingcartLeve, echo=FALSE}

plot(fit.cart, scales = list(x = list(rot = 90)))

```

Accuracy decreases after CP = 0.00035.  Complexity Parameter is the minimum improvement in the model needed at each node. The cp value is a stopping parameter. It helps speed up the search for splits because it can identify splits that don’t meet this criteria and prune them before going too far.


Let's look at the Tree how the decision is being calculated:
```{r}

prp(fit.cart$finalModel, box.palette = "Reds", tweak = 1.2)

```

## Deployment of Performance Predictor
We will run a prediction using the Classification Tree Model, then we will generate the model to be used by our Shiny App.
```{r deployment, echo=FALSE}
xnew = sdata[5000,c("priorityN","levelN","app_category","region","impactN","prod_line")]

predictions <- predict(fit.cart,xnew)
xnew$priorityN
head(predictions)
saveRDS(fit.cart,'jjts_perf_model.rds')

```



##Conclusion:
In this work we presented a method for constructing a multi-level classiﬁer to predict performance of a new ticket and what would be the number of support tickets under the different Group Level. We demonstrated that the information present at the lower level can be successfully propagated to the upper level to make reasonable predictions. No additional features other than the Application category, Product Line, Priority, Region, Impact and group level were necessary to predict the performance and Group Level of a new ticket.

Our goal, predicting performance, forced us to collect high quality data and develop a rigorous evaluation procedure. During the evaluation we carefully separated training and testing support tickets to avoid information leak. We identified that Random Forest works well to predict the Level of Support while Classification and Regression Trees (CART) method works well under these conditions better than other methods to detect a multi classification prediction to predict the performance.

There is still room for further improvements regarding classiﬁcation accuracy. We could plan to include additional features both at the vendor level and resource experienced level of the resources to see if our model can beneﬁt from them. Another direction that we want to explore is expanding the model to include other product lines, and evaluating it on bigger datasets. We hope that our work will inspire further discussions at Johnson & Johnson regarding evaluation strategies such as predicting Resolution categories. We believe that deeper understanding of those matters would allow the comparison of diﬀerent methods in a more systematic manner which would be beneﬁcial for the research done in ITSM CSI Continues Service Improvement.

